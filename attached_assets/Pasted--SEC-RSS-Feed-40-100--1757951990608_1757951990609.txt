# ëŒ€ìš©ëŸ‰ ë‚´ë¶€ì ê±°ë˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ì „ í•´ê²°ì±…

## ğŸ”¥ **í˜„ì¬ ë¬¸ì œì  & ì™„ì „í•œ í•´ê²°ì±…**

### í˜„ì¬ ìƒí™©

- SEC RSS Feed â†’ ìµœì‹  40-100ê°œë§Œ ì œê³µ
- íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€
- Rate Limiting & WAF ì°¨ë‹¨
- **ì‹¤ì œë¡œëŠ” ìˆ˜ë§Œ ê°œ ë°ì´í„°ê°€ ìˆëŠ”ë° ëª‡ ê°œë§Œ ë‚˜ì˜´**

### í•´ê²°ì±…: **ë²Œí¬ ë°ì´í„° ìˆ˜ì§‘ + ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**

## ğŸ’¾ **ë°©ë²• 1: SEC ê³µì‹ ë°ì´í„°ì…‹ ë²Œí¬ ë‹¤ìš´ë¡œë“œ (ë¬´ë£Œ)**

### A. SEC ê³µì‹ ë°ì´í„°ì…‹ í™œìš©

```python
# SECì—ì„œ ì œê³µí•˜ëŠ” ì™„ì „í•œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
import requests
import pandas as pd
from io import StringIO

def download_sec_bulk_data():
    """
    SECì—ì„œ ì œê³µí•˜ëŠ” ê³µì‹ ë‚´ë¶€ì ê±°ë˜ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    2009ë…„ë¶€í„° í˜„ì¬ê¹Œì§€ ëª¨ë“  ë°ì´í„° í¬í•¨
    """
    
    # SEC ê³µì‹ ë²Œí¬ ë°ì´í„° URLë“¤
    datasets = {
        '2024': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2024q4_insider.zip',
        '2023': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2023q4_insider.zip',
        '2022': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2022q4_insider.zip',
        # ... 2009ë…„ê¹Œì§€ ëª¨ë“  ë…„ë„
    }
    
    all_data = []
    
    for year, url in datasets.items():
        print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {year}ë…„ ë°ì´í„°...")
        
        response = requests.get(url)
        
        # ZIP íŒŒì¼ ì••ì¶• í•´ì œ í›„ íŒŒì‹±
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            # SUBMISSION.tsv: íŒŒì¼ë§ ê¸°ë³¸ ì •ë³´
            # DERIVATIVETABLE.tsv: ì˜µì…˜ ë“± íŒŒìƒìƒí’ˆ ê±°ë˜
            # NONDERIVATIVETABLE.tsv: ì£¼ì‹ ë“± ì¼ë°˜ ì¦ê¶Œ ê±°ë˜
            
            with zip_file.open('NONDERIVATIVETABLE.tsv') as file:
                df = pd.read_csv(file, sep='\t', low_memory=False)
                all_data.append(df)
                print(f"âœ… {year}ë…„: {len(df):,}ê°œ ê±°ë˜ ë¡œë“œ")
    
    # ëª¨ë“  ë°ì´í„° ê²°í•©
    complete_data = pd.concat(all_data, ignore_index=True)
    print(f"ğŸ‰ ì´ {len(complete_data):,}ê°œ ê±°ë˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    
    return complete_data

# ì‹¤í–‰
insider_data = download_sec_bulk_data()
# ê²°ê³¼: ìˆ˜ì‹­ë§Œ ê°œì˜ ê±°ë˜ ë°ì´í„°!
```

### B. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì €ì¥

```python
def process_and_store_data(raw_data):
    """
    ì›ì‹œ SEC ë°ì´í„°ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ ë³€í™˜
    """
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ ë° ì´ë¦„ ë³€ê²½
    processed_data = raw_data[[
        'issuerName',  # íšŒì‚¬ëª…
        'issuerTradingSymbol',  # í‹°ì»¤
        'rptOwnerName',  # ë‚´ë¶€ì ì´ë¦„
        'officerTitle',  # ì§ì±…
        'transactionDate',  # ê±°ë˜ì¼
        'transactionCode',  # ê±°ë˜ ìœ í˜• (P=ë§¤ìˆ˜, S=ë§¤ë„)
        'transactionShares',  # ì£¼ì‹ ìˆ˜
        'transactionPricePerShare',  # ì£¼ë‹¹ ê°€ê²©
        'sharesOwnedFollowingTransaction'  # ê±°ë˜ í›„ ë³´ìœ  ì£¼ì‹
    ]].rename(columns={
        'issuerName': 'company_name',
        'issuerTradingSymbol': 'ticker',
        'rptOwnerName': 'insider_name',
        'officerTitle': 'position',
        'transactionDate': 'trade_date',
        'transactionCode': 'trade_type',
        'transactionShares': 'shares',
        'transactionPricePerShare': 'price',
        'sharesOwnedFollowingTransaction': 'shares_owned'
    })
    
    # ë°ì´í„° ì •ë¦¬
    processed_data['trade_type'] = processed_data['trade_type'].map({
        'P': 'Purchase',
        'S': 'Sale',
        'A': 'Award',
        'F': 'Payment of Exercise Price',
        'M': 'Exercise'
    })
    
    # ì´ ê±°ë˜ ê¸ˆì•¡ ê³„ì‚°
    processed_data['total_value'] = processed_data['shares'] * processed_data['price']
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    processed_data['trade_date'] = pd.to_datetime(processed_data['trade_date'])
    
    # NULL ê°’ ì²˜ë¦¬
    processed_data = processed_data.dropna(subset=['company_name', 'ticker', 'insider_name'])
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_data):,}ê°œ ê±°ë˜")
    
    return processed_data

# PostgreSQLì— ì €ì¥
def save_to_database(data):
    """
    ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ PostgreSQLì— ë²Œí¬ ì‚½ì…
    """
    from sqlalchemy import create_engine
    
    engine = create_engine(DATABASE_URL)
    
    # ë²Œí¬ ì‚½ì… (ìˆ˜ì‹­ë§Œ ê°œ í•œ ë²ˆì— ì²˜ë¦¬)
    data.to_sql('insider_trades', engine, if_exists='replace', index=False, method='multi')
    
    print(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì— {len(data):,}ê°œ ê±°ë˜ ì €ì¥ ì™„ë£Œ!")

# ì‹¤í–‰
processed_data = process_and_store_data(insider_data)
save_to_database(processed_data)
```

## ğŸš€ **ë°©ë²• 2: OpenInsider ì™„ì „ ìŠ¤í¬ë˜í•‘ (GitHub ì†”ë£¨ì…˜)**

```python
# ê²€ì¦ëœ ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤í¬ë˜í¼ í™œìš©
import requests
import pandas as pd
from bs4 import BeautifulSoup
import time

class OpenInsiderScraper:
    def __init__(self):
        self.base_url = "http://openinsider.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_all_pages(self, max_pages=1000):
        """
        OpenInsiderì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘
        ì‹¤ì œë¡œ ìˆ˜ë§Œ ê°œì˜ ê±°ë˜ ë°ì´í„° ìˆ˜ì§‘ ê°€ëŠ¥
        """
        all_trades = []
        
        for page in range(1, max_pages + 1):
            print(f"ìŠ¤í¬ë˜í•‘ ì¤‘: í˜ì´ì§€ {page}")
            
            url = f"{self.base_url}/screener?s=&o=&pl=&ph=&ll=&lh=&fd=730&fdr=&td=0&tdr=&fdlyl=&fdlyh=&daysago=&xp=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=100&page={page}"
            
            try:
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                table = soup.find('table', class_='tinytable')
                
                if not table:
                    print(f"í˜ì´ì§€ {page}: ë” ì´ìƒ ë°ì´í„° ì—†ìŒ")
                    break
                
                # í…Œì´ë¸” ë°ì´í„° íŒŒì‹±
                rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                
                page_trades = []
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 10:  # ì¶©ë¶„í•œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ
                        trade = {
                            'filing_date': cells[1].text.strip(),
                            'company': cells[2].text.strip(),
                            'ticker': cells[3].text.strip(),
                            'insider': cells[4].text.strip(),
                            'position': cells[5].text.strip(),
                            'trade_type': cells[6].text.strip(),
                            'shares': self.parse_number(cells[7].text.strip()),
                            'price': self.parse_number(cells[8].text.strip()),
                            'total_value': self.parse_number(cells[9].text.strip())
                        }
                        page_trades.append(trade)
                
                all_trades.extend(page_trades)
                print(f"í˜ì´ì§€ {page}: {len(page_trades)}ê°œ ê±°ë˜ ìˆ˜ì§‘")
                
                # Rate limiting ì¤€ìˆ˜
                time.sleep(1)
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                continue
        
        print(f"ğŸ‰ ì´ {len(all_trades):,}ê°œ ê±°ë˜ ìˆ˜ì§‘ ì™„ë£Œ!")
        return pd.DataFrame(all_trades)
    
    def parse_number(self, text):
        """ìˆ«ì í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ ìˆ«ìë¡œ ë³€í™˜"""
        if not text or text == '-':
            return None
        
        # $, ì‰¼í‘œ ì œê±°
        text = text.replace('$', '').replace(',', '')
        
        # K, M, B ë‹¨ìœ„ ì²˜ë¦¬
        if 'K' in text:
            return float(text.replace('K', '')) * 1000
        elif 'M' in text:
            return float(text.replace('M', '')) * 1000000
        elif 'B' in text:
            return float(text.replace('B', '')) * 1000000000
        
        try:
            return float(text)
        except:
            return None

# ì‹¤í–‰
scraper = OpenInsiderScraper()
complete_data = scraper.scrape_all_pages(max_pages=500)  # 5ë§Œê°œ+ ê±°ë˜ ë°ì´í„°

print(f"ìµœì¢… ìˆ˜ì§‘: {len(complete_data):,}ê°œ ê±°ë˜")
```

## âš¡ **ë°©ë²• 3: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ (ì¶”ì²œ)**

```python
def hybrid_data_collection():
    """
    ìµœê³  íš¨ìœ¨ì„±: ë²Œí¬ ë‹¤ìš´ë¡œë“œ + ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì¡°í•©
    """
    
    # 1. íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°: SEC ë²Œí¬ ë°ì´í„°ì…‹
    print("1ë‹¨ê³„: íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘...")
    historical_data = download_sec_bulk_data()
    save_to_database(historical_data)
    
    # 2. ìµœì‹  ë°ì´í„°: OpenInsider ìŠ¤í¬ë˜í•‘
    print("2ë‹¨ê³„: ìµœì‹  ë°ì´í„° ì—…ë°ì´íŠ¸...")
    scraper = OpenInsiderScraper()
    recent_data = scraper.scrape_all_pages(max_pages=50)  # ìµœê·¼ 5000ê°œ
    
    # 3. ì¤‘ë³µ ì œê±° í›„ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
    print("3ë‹¨ê³„: ë°ì´í„° ë³‘í•© ë° ì¤‘ë³µ ì œê±°...")
    update_database_with_recent_data(recent_data)
    
    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘ ì™„ë£Œ!")

def update_database_with_recent_data(new_data):
    """
    ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    """
    from sqlalchemy import create_engine
    
    engine = create_engine(DATABASE_URL)
    
    # UPSERT ì¿¼ë¦¬ë¡œ ì¤‘ë³µ ë°©ì§€
    for _, trade in new_data.iterrows():
        query = """
        INSERT INTO insider_trades (company_name, ticker, insider_name, position, 
                                   trade_date, trade_type, shares, price, total_value)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (company_name, ticker, insider_name, trade_date, shares) 
        DO NOTHING
        """
        
        engine.execute(query, tuple(trade.values))

# ì‹¤í–‰
hybrid_data_collection()
```

## ğŸ¯ **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…**

### **ì˜¤ëŠ˜ ë‹¹ì¥ ì‹œë„í•´ë³¼ ê²ƒ:**

1. **SEC ë²Œí¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ**:
   
   ```bash
   # 2024ë…„ ì „ì²´ ë°ì´í„° (ìˆ˜ì‹­ë§Œ ê°œ ê±°ë˜)
   curl -o insider_2024.zip https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2024q3_insider.zip
   ```
1. **OpenInsider GitHub ìŠ¤í¬ë˜í¼ ì‚¬ìš©**:
   
   ```bash
   git clone https://github.com/sd3v/openinsiderData
   cd openinsiderData
   python scraper.py
   ```
1. **ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ Python ìŠ¤í¬ë¦½íŠ¸**:
   ìœ„ì˜ `OpenInsiderScraper` í´ë˜ìŠ¤ë¥¼ Replitì—ì„œ ë°”ë¡œ ì‹¤í–‰

## ğŸ’¡ **ì™œ ê¸°ì¡´ ë°©ì‹ì´ ì‹¤íŒ¨í–ˆëŠ”ì§€**

1. **RSS Feed ì œí•œ**: SEC RSSëŠ” ìµœì‹  40-100ê°œë§Œ ì œê³µ
1. **í˜ì´ì§€ë„¤ì´ì…˜ ë¬´ì‹œ**: OpenInsiderëŠ” ìˆ˜ì²œ í˜ì´ì§€ê°€ ìˆëŠ”ë° ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§
1. **Rate Limiting**: ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ìœ¼ë¡œ ì°¨ë‹¨ë‹¹í•¨
1. **ë°ì´í„° êµ¬ì¡° ì´í•´ ë¶€ì¡±**: SECì˜ ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì§€ ëª»í•¨

## ğŸš€ **ê²°ê³¼ ì˜ˆìƒ**

ì´ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•˜ë©´:

- **íˆìŠ¤í† ë¦¬ì»¬**: 2009ë…„ë¶€í„° í˜„ì¬ê¹Œì§€ **ìˆ˜ì‹­ë§Œ ê°œ** ê±°ë˜
- **ì‹¤ì‹œê°„**: ë§¤ì¼ ìˆ˜ë°±~ìˆ˜ì²œ ê°œ ì‹ ê·œ ê±°ë˜ ì—…ë°ì´íŠ¸
- **ì™„ì „ì„±**: ëˆ„ë½ ì—†ëŠ” ì™„ì „í•œ ë°ì´í„°ì…‹

**ê²°ë¡ **: RSS í¬ë¡¤ë§ì„ í¬ê¸°í•˜ê³  **ë²Œí¬ ë‹¤ìš´ë¡œë“œ + ì „ë©´ ìŠ¤í¬ë˜í•‘**ìœ¼ë¡œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤!