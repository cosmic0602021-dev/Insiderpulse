# 대용량 내부자 거래 데이터 수집 완전 해결책

## 🔥 **현재 문제점 & 완전한 해결책**

### 현재 상황

- SEC RSS Feed → 최신 40-100개만 제공
- 히스토리컬 데이터 접근 불가
- Rate Limiting & WAF 차단
- **실제로는 수만 개 데이터가 있는데 몇 개만 나옴**

### 해결책: **벌크 데이터 수집 + 실시간 업데이트**

## 💾 **방법 1: SEC 공식 데이터셋 벌크 다운로드 (무료)**

### A. SEC 공식 데이터셋 활용

```python
# SEC에서 제공하는 완전한 데이터셋 다운로드
import requests
import pandas as pd
from io import StringIO

def download_sec_bulk_data():
    """
    SEC에서 제공하는 공식 내부자 거래 데이터셋 다운로드
    2009년부터 현재까지 모든 데이터 포함
    """
    
    # SEC 공식 벌크 데이터 URL들
    datasets = {
        '2024': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2024q4_insider.zip',
        '2023': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2023q4_insider.zip',
        '2022': 'https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2022q4_insider.zip',
        # ... 2009년까지 모든 년도
    }
    
    all_data = []
    
    for year, url in datasets.items():
        print(f"다운로드 중: {year}년 데이터...")
        
        response = requests.get(url)
        
        # ZIP 파일 압축 해제 후 파싱
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            # SUBMISSION.tsv: 파일링 기본 정보
            # DERIVATIVETABLE.tsv: 옵션 등 파생상품 거래
            # NONDERIVATIVETABLE.tsv: 주식 등 일반 증권 거래
            
            with zip_file.open('NONDERIVATIVETABLE.tsv') as file:
                df = pd.read_csv(file, sep='\t', low_memory=False)
                all_data.append(df)
                print(f"✅ {year}년: {len(df):,}개 거래 로드")
    
    # 모든 데이터 결합
    complete_data = pd.concat(all_data, ignore_index=True)
    print(f"🎉 총 {len(complete_data):,}개 거래 데이터 수집 완료!")
    
    return complete_data

# 실행
insider_data = download_sec_bulk_data()
# 결과: 수십만 개의 거래 데이터!
```

### B. 데이터 전처리 및 저장

```python
def process_and_store_data(raw_data):
    """
    원시 SEC 데이터를 사용자 친화적 형태로 변환
    """
    
    # 필요한 컬럼만 추출 및 이름 변경
    processed_data = raw_data[[
        'issuerName',  # 회사명
        'issuerTradingSymbol',  # 티커
        'rptOwnerName',  # 내부자 이름
        'officerTitle',  # 직책
        'transactionDate',  # 거래일
        'transactionCode',  # 거래 유형 (P=매수, S=매도)
        'transactionShares',  # 주식 수
        'transactionPricePerShare',  # 주당 가격
        'sharesOwnedFollowingTransaction'  # 거래 후 보유 주식
    ]].rename(columns={
        'issuerName': 'company_name',
        'issuerTradingSymbol': 'ticker',
        'rptOwnerName': 'insider_name',
        'officerTitle': 'position',
        'transactionDate': 'trade_date',
        'transactionCode': 'trade_type',
        'transactionShares': 'shares',
        'transactionPricePerShare': 'price',
        'sharesOwnedFollowingTransaction': 'shares_owned'
    })
    
    # 데이터 정리
    processed_data['trade_type'] = processed_data['trade_type'].map({
        'P': 'Purchase',
        'S': 'Sale',
        'A': 'Award',
        'F': 'Payment of Exercise Price',
        'M': 'Exercise'
    })
    
    # 총 거래 금액 계산
    processed_data['total_value'] = processed_data['shares'] * processed_data['price']
    
    # 날짜 형식 변환
    processed_data['trade_date'] = pd.to_datetime(processed_data['trade_date'])
    
    # NULL 값 처리
    processed_data = processed_data.dropna(subset=['company_name', 'ticker', 'insider_name'])
    
    print(f"전처리 완료: {len(processed_data):,}개 거래")
    
    return processed_data

# PostgreSQL에 저장
def save_to_database(data):
    """
    처리된 데이터를 PostgreSQL에 벌크 삽입
    """
    from sqlalchemy import create_engine
    
    engine = create_engine(DATABASE_URL)
    
    # 벌크 삽입 (수십만 개 한 번에 처리)
    data.to_sql('insider_trades', engine, if_exists='replace', index=False, method='multi')
    
    print(f"✅ 데이터베이스에 {len(data):,}개 거래 저장 완료!")

# 실행
processed_data = process_and_store_data(insider_data)
save_to_database(processed_data)
```

## 🚀 **방법 2: OpenInsider 완전 스크래핑 (GitHub 솔루션)**

```python
# 검증된 오픈소스 스크래퍼 활용
import requests
import pandas as pd
from bs4 import BeautifulSoup
import time

class OpenInsiderScraper:
    def __init__(self):
        self.base_url = "http://openinsider.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_all_pages(self, max_pages=1000):
        """
        OpenInsider의 모든 페이지를 스크래핑
        실제로 수만 개의 거래 데이터 수집 가능
        """
        all_trades = []
        
        for page in range(1, max_pages + 1):
            print(f"스크래핑 중: 페이지 {page}")
            
            url = f"{self.base_url}/screener?s=&o=&pl=&ph=&ll=&lh=&fd=730&fdr=&td=0&tdr=&fdlyl=&fdlyh=&daysago=&xp=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=100&page={page}"
            
            try:
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                table = soup.find('table', class_='tinytable')
                
                if not table:
                    print(f"페이지 {page}: 더 이상 데이터 없음")
                    break
                
                # 테이블 데이터 파싱
                rows = table.find_all('tr')[1:]  # 헤더 제외
                
                page_trades = []
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 10:  # 충분한 컬럼이 있는 경우만
                        trade = {
                            'filing_date': cells[1].text.strip(),
                            'company': cells[2].text.strip(),
                            'ticker': cells[3].text.strip(),
                            'insider': cells[4].text.strip(),
                            'position': cells[5].text.strip(),
                            'trade_type': cells[6].text.strip(),
                            'shares': self.parse_number(cells[7].text.strip()),
                            'price': self.parse_number(cells[8].text.strip()),
                            'total_value': self.parse_number(cells[9].text.strip())
                        }
                        page_trades.append(trade)
                
                all_trades.extend(page_trades)
                print(f"페이지 {page}: {len(page_trades)}개 거래 수집")
                
                # Rate limiting 준수
                time.sleep(1)
                
            except Exception as e:
                print(f"페이지 {page} 오류: {e}")
                continue
        
        print(f"🎉 총 {len(all_trades):,}개 거래 수집 완료!")
        return pd.DataFrame(all_trades)
    
    def parse_number(self, text):
        """숫자 텍스트를 실제 숫자로 변환"""
        if not text or text == '-':
            return None
        
        # $, 쉼표 제거
        text = text.replace('$', '').replace(',', '')
        
        # K, M, B 단위 처리
        if 'K' in text:
            return float(text.replace('K', '')) * 1000
        elif 'M' in text:
            return float(text.replace('M', '')) * 1000000
        elif 'B' in text:
            return float(text.replace('B', '')) * 1000000000
        
        try:
            return float(text)
        except:
            return None

# 실행
scraper = OpenInsiderScraper()
complete_data = scraper.scrape_all_pages(max_pages=500)  # 5만개+ 거래 데이터

print(f"최종 수집: {len(complete_data):,}개 거래")
```

## ⚡ **방법 3: 하이브리드 접근 (추천)**

```python
def hybrid_data_collection():
    """
    최고 효율성: 벌크 다운로드 + 실시간 업데이트 조합
    """
    
    # 1. 히스토리컬 데이터: SEC 벌크 데이터셋
    print("1단계: 히스토리컬 데이터 수집...")
    historical_data = download_sec_bulk_data()
    save_to_database(historical_data)
    
    # 2. 최신 데이터: OpenInsider 스크래핑
    print("2단계: 최신 데이터 업데이트...")
    scraper = OpenInsiderScraper()
    recent_data = scraper.scrape_all_pages(max_pages=50)  # 최근 5000개
    
    # 3. 중복 제거 후 데이터베이스 업데이트
    print("3단계: 데이터 병합 및 중복 제거...")
    update_database_with_recent_data(recent_data)
    
    print("✅ 하이브리드 수집 완료!")

def update_database_with_recent_data(new_data):
    """
    새로운 데이터만 데이터베이스에 추가 (중복 방지)
    """
    from sqlalchemy import create_engine
    
    engine = create_engine(DATABASE_URL)
    
    # UPSERT 쿼리로 중복 방지
    for _, trade in new_data.iterrows():
        query = """
        INSERT INTO insider_trades (company_name, ticker, insider_name, position, 
                                   trade_date, trade_type, shares, price, total_value)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (company_name, ticker, insider_name, trade_date, shares) 
        DO NOTHING
        """
        
        engine.execute(query, tuple(trade.values))

# 실행
hybrid_data_collection()
```

## 🎯 **즉시 실행 가능한 해결책**

### **오늘 당장 시도해볼 것:**

1. **SEC 벌크 데이터 다운로드**:
   
   ```bash
   # 2024년 전체 데이터 (수십만 개 거래)
   curl -o insider_2024.zip https://www.sec.gov/files/structureddata/data/insider-transactions-data-sets/2024q3_insider.zip
   ```
1. **OpenInsider GitHub 스크래퍼 사용**:
   
   ```bash
   git clone https://github.com/sd3v/openinsiderData
   cd openinsiderData
   python scraper.py
   ```
1. **즉시 적용 가능한 Python 스크립트**:
   위의 `OpenInsiderScraper` 클래스를 Replit에서 바로 실행

## 💡 **왜 기존 방식이 실패했는지**

1. **RSS Feed 제한**: SEC RSS는 최신 40-100개만 제공
1. **페이지네이션 무시**: OpenInsider는 수천 페이지가 있는데 첫 페이지만 크롤링
1. **Rate Limiting**: 너무 빠른 요청으로 차단당함
1. **데이터 구조 이해 부족**: SEC의 실제 데이터 구조를 파악하지 못함

## 🚀 **결과 예상**

이 방법들을 사용하면:

- **히스토리컬**: 2009년부터 현재까지 **수십만 개** 거래
- **실시간**: 매일 수백~수천 개 신규 거래 업데이트
- **완전성**: 누락 없는 완전한 데이터셋

**결론**: RSS 크롤링을 포기하고 **벌크 다운로드 + 전면 스크래핑**으로 바꾸면 됩니다!